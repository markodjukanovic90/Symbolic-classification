%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% https://www.biomedcentral.com/getpublished                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% https://miktex.org/                                             %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
\usepackage{amsthm,amsmath}
\usepackage{tabularx}
\usepackage{xspace}
\usepackage{color}
\usepackage{epsfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{tikz}
%\usepackage{fullpage}
\usepackage{calc}
\usetikzlibrary{positioning,shadows,arrows,trees,shapes,fit}
\usepackage{blindtext}
\usepackage{pgfplots}
%\pgfplotsset{compat=1.16}
\usepackage{pythonhighlight}
\usepackage{fixme}
\fxsetup{status=draft} % <====== add this line
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
  
\usepackage{url}
\raggedbottom
%\algnewcommand\algorithmicforeach{\textbf{for each}}
%\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

% scaling factor for tables
\newcommand\tabscale{0.8}
\newtheorem{definition}{Definition}
\newtheorem{Lemma}{Lemma}


\usepackage{fixme}

  
%\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\def\includegraphic{}
%\def\includegraphics{}

%%% Put your definitions there:
\startlocaldefs
\endlocaldefs

%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Symbolic classification with RILS-ROLS and HROCH algorithms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
  addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
  corref={aff1},                       % id of corresponding address, if any
% noteref={n1},                        % id's of article notes, if any
  email={kartelj@matf.bg.ac.rs}   % email address
]{\inits{A.K.}\fnm{Aleksandar} \snm{Kartelj}}
\author[
  addressref={aff2},
  email={marko.djukanovic@pmf.unibl.org}
]{\inits{M.DJ.}\fnm{Marko} \snm{DjukanoviÄ‡}}

\author[
email={jan.pigos@gmail.com}
]{\inits{M.DJ.}\fnm{Jan} \snm{Pigos}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgdiv{Department of Informatics, Faculty of Mathematics},             % department, if any
  \orgname{University of Belgrade},          % university, etc
  \city{Belgrade},                              % city
  \cny{Serbia}                                    % country
}
\address[id=aff2]{%
  \orgdiv{Faculty of Sciences and Mathematics},
  \orgname{University of Banja Luka},
  %\street{},
  %\postcode{}
  \city{Banja Luka},
  \cny{Bosnia and Herzegovina}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{artnotes}
%%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
%\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                           %%
%% The Abstract begins here                  %%
%%                                           %%
%% Please refer to the Instructions for      %%
%% authors on https://www.biomedcentral.com/ %%
%% and include the section headings          %%
%% accordingly for your article type.        %%
%%                                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
In this paper, we solve the symbolic classification problem. TODO
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{symbolic classification}
\kwd{iterated local search}
\kwd{ordinary least squares}
\kwd{hill climbing}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for two column layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                            %%
%% The Main Body begins here                  %%
%%                                            %%
%% Please refer to the instructions for       %%
%% authors on:                                %%
%% https://www.biomedcentral.com/getpublished %%
%% and include the section headings           %%
%% accordingly for your article type.         %%
%%                                            %%
%% See the Results and Discussion section     %%
%% for details on how to create sub-sections  %%
%%                                            %%
%% use \cite{...} to cite references          %%
%%  \cite{koon} and                           %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}      %%
%%                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
	\section{Introduction}\label{sec:introduction}

The problem of symbolic regression (SR)~\cite{billard2002symbolic} is one of a central problem in the filed of machine learning (ML) and mathematical optimization; it has attracted many researchers to study it over the last few decades. 
SR can be seen as  a type of regression analysis; it  searches a mathematical expressions $f$, called a model,  that best fits to the data; more precisely,  given is a set of input data $\textbf{X}$ and a set of target variables $\textbf{y}$, we aim at a mathematical formula $f$ that explains the target variable in terms of the input data, that is $f(\textbf{X}) = \textbf{y}$. In essence, a good candidate solution $f$ must be accurate enough to the output variables as well simple enough. The problem itself can be seen as a generalization of specific variants of regression where the desired functional form of model $f$ is pre-assumed, e.g., the well-known linear regression considers only linear-form models, while polynomial regression allows any polynomial for a feasible model~\cite{stimson1978interpreting}, etc. SR problem can be seen as an of-shot of the Genetic programming (GP) methodology introduced by Koza~\cite{koza1994genetic}.  SR has received many attention being solved by a few dozens of algorithms such as Genetic programming~\cite{augusto2000symbolic}, a Bee colony optimization algorithm~\cite{karaboga2012artificial}, a divide-and-conquer based approaches~\cite{udrescu2020ai}, an iterated greedy--based method RILS-ROLS~\cite{kartelj2023rils}, just to name a few.  The later approach is proven to be  the current state-of-the-art on the ground-truth SR problems from literature, that are collected in the two well-known benchmark sets, labelled Feynman and Strogatz. It is worth mentioning that when comes to solving  black-box SR, that is, when the task is to seek a best fitting model on data quantified by, e.g.,  $R^2$ value (or some other measure of success) without knowing what the real model is, the best performing algorithms from literature are  Operon~\cite{burlacu2020operon}, SBP-GP~\cite{virgolin2019linear}, followed by the above-mentioned RILS-ROLS. By popularization of SR among the researchers, uniform, robust, and transparent
benchmarking standards were not known until the work of  La Cava et al.~\cite{la2021contemporary}, who  introduced an open-source, reproducible benchmarking platform \texttt{SRBench}. The authors extended  PMLB~\cite{olson2017pmlb} with a newly 130 SR datasets for which exact models are known. \texttt{SRBench} can execute 14 SR methods and 7 ML methods on the set of 252  regression problems. The rest (122) SR datasets belong to the class of  \emph{black-box} problems (the ground-truth is unknown).

Nowadays, SR is pushed into the rank of moderate-to-extreme accuracy.  It has proven its strength and applicability in solving many problems from various research fields such as material science~\cite{wang2019symbolic}, wind speed forecasting~\cite{abdellaoui2021symbolic},  highway crash prediction~\cite{veran2023interpretable}, language assessment~\cite{aryadoust2015application}, etc.  Solving SR is known to be NP-hard in general~\cite{virgolin2022symbolic}. 


Classification belongs to the field of supervised learning and many algorithms have been proposed to tackle various classes of classification problems (binary, multinomial, multi-class, etc.), see, for example~\cite{cunningham2008supervised, caruana2006empirical}. When comes to the \emph{symbolic classification} (SC) problem, the task here is to finding a suitable model that predicts discrete target class using predictor variables that are both discrete or continuous.  SC is directly applied on, for example, an open-source GIS environment for producing a dichotomous maps of landslide susceptibility outputs which classifies the areas on landslide and non-landslide areas, so called topographical attribute landslide susceptibility,   see~\cite{gorsevski2021evolutionary}. 

 Considerably less work has been done for SC in contrast to SR. Note that solving SR efficiently is just a half of a way to achieve it also for SC. It is known that directly applying GP is not in particular successful for solving the multi-class classification problems especially when comes to the problems with a high number of classes~\cite{korns2018evolutionary}. A survey paper on applying GP to solve (binary and multi-class) classification problems is provided in~\cite{espejo2009survey}.  In the last decade, several papers have been published which succeed to raised the SC accuracy in the level to complete with the existing commercial classification tools. These tools are based on evolutionary algorithms~\cite{korns2018evolutionary}, a Multidimensional genetic programming based approach~\cite{munoz2015m3gp,la2019multidimensional},  evolutionary discriminant analysis~\cite{korns2017evolutionary}, etc.  

In this paper, motivated by the success of solving SR efficiently provided by the recent studies, we propose two new classification algorithms that solve the problem of SC: ($i$) RILS-ROLS classifier, and ($ii$) HROCH classifier. 


\fxnote{TODO: say something more about these methods!}

%All regression models have the same goal: given a set of $n$-dimensional input data and its corresponding continuous output target variable, the aim is to find a  mathematical expression (function) of $n$ (input) variables that best \emph{fits} the target variable.  %This computationally intensive task is in general provenly NP--hard~\cite{virgolin2022symbolic}. 
%In the case of linear regression, the model is always a linear combination of input variables. This is in general not good enough, since the target variable might be dependent on a nonlinear function among input variables. Unlike linear regression, SR allows for the search over much larger space of possible mathematical formulas to find the best-fitting ones, i.e., those able to predict the target variable from input variables. The basis of constructing an explicit formula is in elementary operations like addition and multiplication, as well as polynomial, trigonometric, exponential, and other operations.  
%Application: https://towardsdatascience.com/real-world-applications-of-symbolic-regression-2025d17b88ef
 
 

\subsection{Contributions}\label{sec:contibutions}

The main contributions are as follows: 

\begin{enumerate}
	\item We contributed to the field of Symbolic classification by providing two more classification algorithms at disposal. 
	
	\item The proposed methods are evaluated against state-of-the-art classification algorithms as well as a few state-of-the-art SC algorithms from literature. 
	\item \fxnote{TODO}
\end{enumerate}
 
\section{Definitions and notation} \label{sec:search-space}

In this section we formally define the SR problem.

\begin{definition}[Symbolic regression]\label{dfn:sr}
	Given is a dataset $D = \{(\mathbf{x_i}, y_i)\}_{i=1}^n$, where $\mathbf{x_i} \in \mathbb{R}^d$ represents the $i$-th feature (input) vector and $y_i \in \mathbb{R}$ is its corresponding target (output) variable. Suppose the existence of an analytical model of the form $s(\mathbf{x})= g^*(\mathbf{x}, \theta^*) + \epsilon $ that can generate  all observations from $D$.  
	SR aims at learning $\tilde{s}(\mathbf{x})=  \tilde{g}(\mathbf{x}, \tilde{\theta})  \colon \mathbb{R}^d \mapsto \mathbb{R}$  estimated by searching through the space of (mathematical) expressions  $\tilde{g}$ and parameters $\tilde{\theta}$ where  $\epsilon$ is the observed white noise within the given input data. 
	
\end{definition}


\begin{definition}[Symbolic classification]\label{dfn:sc}
	Given is a dataset $D = \{(\mathbf{x_i}, c_i)\}_{i=1}^n$, where $\mathbf{x_i}$ represents the $i$-th feature vector, and $c_i$  dependent unordered categorical
	 value from e.g., $\{1, \ldots, K  \} $, $K\geq 2. $ The aim of SC is to find $K$ (mathematical) expressions $s^i, i=1, \ldots, K$, so that $s(\mathbf{x_i}) = T( s^1\mathbf{x_i}), \ldots, s^K(\mathbf{x_i})  ) = c_i,$ for $i=1, \ldots, K$ where the classification is performed with $T$ which can be based on a threshold  or some other transformation technique, e.g., applying \textrm{argmax}. 
  \end{definition}
 
 
\emph{Notation}. Let $s \colon \mathbb{R}^d \mapsto \mathbb{R}$ be a model for data $D$ from Definition~\ref{dfn:sr}.  We introduce the following notation for SR. 
 
\begin{equation}
		R^2(s) = 1- \frac{ \sum_{i=1}^{n} \left( s(\mathbf{x_i}) - y_i \right)^2  }{ \sum_{i=1}^{n} \left( y_{i}  - \overline{y} \right)^2  }
	\end{equation}
	\begin{equation}
		RMSE(s) = \sqrt {\frac{1}{n} \sum_{i=1}^{n} (s(\mathbf{x_i}) - y_{i})^2}
	\end{equation}
	
 Here, $s(\mathbf{x_i})$ for the value predicted by the candidate model $s$ for an input vector $\mathbf{x_i}$, $y_{i}$ for the (exact) target value, and $\overline{y}$ for the mean of all target values.
 
Concerning notation regarding SC, we mention the following (multi-class) metrics  %https://www.evidentlyai.com/classification-metrics/multi-class-metrics
\begin{itemize}
    \item \emph{accuracy} metric: a measure how many observations were correctly classified;
    \item   \emph{precision} metric: one calculate the precision for each class individually, and then to ``average'' the precision  across all classes (use macro or micro averaging).
    \item  \emph{recall} metric: the same calculation as for the precision metric performed but with the \emph{recall}; recall verifies the model's ability of identifying all instances of a particular class. 
    \item $F_1$ metric: the harmonic mean of two score, precision and recall; in the multi-class classification, the $F_1$ score is calculated per class applying one-vs-rest manner. 
     %https://www.baeldung.com/cs/multi-class-f1-score
       
\end{itemize}
 
 	\section{The proposed algorithms}\label{sec:rils}

 This section presents two new classification algorithms for the problem of Symbolic classification. The first one is based on the state-of-the-art method to solving ground-truth-based SR, whilst the second one is a new \textsc{Hroch} method that is based on a classical hill-climbing methodology utilizing a high degree of parallelism in there. 
 

\subsection{Search space of symbolic regression} 
 
Koza~\cite{koza1994genetic} pioneered the exploration of Symbolic Regression (SR) as a specific application of Genetic programming (GP). Genetic programming is a versatile approach applicable to the optimization of nonlinear structures, such as computer programs. Programs are represented using syntax trees composed of functions/operations involving input features and constants. A concrete illustration of a function represented by a syntax tree is provided in Figure~\ref{fig:syntax-tree-example}. Fundamentally, any valid syntax trees constitutes the solution search space for SR. In this context, each sample model $\tilde{f}$ corresponds to a distinct point in the search space, represented by a corresponding syntax tree.

The accuracy of a solution can be assessed based on historical data $D$ and a selected error metric, such as $MSE$, $RMSE$, $R^2$, or a combination thereof. Notably, the SR search space exhibits a dual nature: it is both discrete and continuous. While primarily conceptualized as a discrete optimization problem due to the countable number of possible solution functional forms, it also encompasses elements addressable through continuous (global) optimization. This, for example, includes the fitting of constants (coefficients). Standard elementary mathematical functions commonly employed in this context include $\sqrt{x}$, $x^2$, $\sin$, $\cos$, $\log$, $\exp$, $\arcsin$, $\arccos$, $a^x$, complemented by a set of standard arithmetic operators: $+$, $-$, $\cdot$, and $/$.
 
 \begin{figure}[!ht]
 	\centering
 	\begin{tikzpicture}
 		[font=\small, edge from parent, 
 		every node/.style={top color=white, bottom color=blue!25, 
 			rectangle,rounded corners, minimum size=6mm, draw=blue!75,
 			very thick, drop shadow, align=center},
 		edge from parent/.style={draw=blue!50,thick},
 		level 1/.style={sibling distance=3cm},
 		level 2/.style={sibling distance=1.2cm}, 
 		level 3/.style={sibling distance=1cm}, 
 		level distance=2cm,
 		]
 		\node (A) {$\cdot$} 
 		child { node (B) {$\log$}
 			%child { node {x} 
 				%edge from parent node[left=.5em,draw=none] {} }
 			child { node {$x$}}
 		}
 		child {node (C) {$\cdot$}
 			%child { node {x}
 				%	child { node {C1a}}
 				%}
 			child { node {$x$}}
 			child { node {$+$}
 				child { node {$x$}}
 				child { node {$y$} %edge from parent node[right=.5em,draw=none] {$\frac{a}{b}$}
 				}
 			}
 		};
 		%	child { node {+} 
 			%	child { node {2}}
 			%	child { node {$x$}}
 			%};
 		
 	\end{tikzpicture}
 	
 	\caption{Syntax tree representation for the expression $\log{x} \cdot   ( x \cdot ( x + y )Ë˜)$}
 	\label{fig:syntax-tree-example}
 \end{figure}
 

\subsection{The \textsc{RILS}-\textsc{ROLS}  classifier}

In this section we first describe the basic components of the \textsc{Rils}-\textsc{Rols} algorithm, the main ingredient of the \textsc{Rils-Rols} classifier. For all the details on this method, we refer readers to the paper~\cite{kartelj2023rils}. 

 \textsc{Rils}-\textsc{Rols} combines the popular iterated local search meta-heuristic (ILS)~\cite{lourencco2003iterated,lourencco2010iterated} with the ordinary least square method (OLS)~\cite{leng2007ordinary}.  Iterated local search serves as a method backbone,  mainly tackling combinatorial aspects of the problem consisting of searching for essential functional forms of good model representations which are additionally tuned later in the search. OLS algorithm tackles continuous aspects of the problem, that is it efficiently determines best-fitting coefficients of  linear  combinations within solution equations. One of the  important steps of the algorithm is generating perturbations in the vicinity of a solution, i.e. the set of solutions  which are with a ``distance'' of 1 (called 1-perturbation). This is done by making simple changes on the \emph{per-node} level of solution's expression tree.  These per-node change operations are predefined in the algorithm, see more about it in~\cite{kartelj2023rils}. After generating the set of all such 1-perturbation trees representing solutions around solution $s$, first all of them are  simplified, and subsequently improved by local search procedure (explained below),  and then examined by calculating their fitness values (explained below). The one solution among them with the best fitness is compared to the fitness of the best-so-far known solution. Note that this is a modification in comparison to the original \textsc{Rils}-\textsc{Rols} implementation that only considers the first-improvement strategy among the set of all 1-perturbation around $s$, where the order of examination was according to their $R^2$ scores. If a new best solution has been found, it becomes a new incumbent ($s_{best}$) and $s=s_{best}$ or otherwise a random solution s' from the set of all 1-perturbations around $s_{best}$ is picked and the set of 1-perturbations are generated around $s'$. Afterwards, a random solution is picked from that set and stored as solution $s$ (representing a 2-perturbation of $s_{best}$) in the next iteration. It is worth mentioning that the algorithm only uses a piece of the data sampled from the input data for searching appropriate model  at each iteration. The size of the data increases (it becomes duplicated w.r.t. the data size of the previous iteration) once there is no improvement in the algorithms after a number of iterations. The algorithm iterates as long as none of the termination criteria are met: ($i$) the maximal running time has been reached; ($ii$) the maximal number of fitness calculations has been made.
  
 
 \textit{Fitness function}. To verify goodness of solutions in the search process, a carefully designed fitness function has been utilized that takes into consideration three model quality scores -- $R^2$, \emph{RMSE} and the size of model, combined into a non-linear function. It is of the following form:
 \begin{equation}\label{eq:fitness-rils-rols} 
 	 fit(s) = (1+ RMSE(s)) \cdot (2-R^2(s)) \cdot (1+ p_{size} \cdot |s|)
 \end{equation}
where  $p_{size}>0$ is a strategic parameter and $|s|$ is the size of solution (model) $s$ -- that is the number of nodes of its tree representation. 

  \emph{Local search}. In order to intensify the search, an efficient local search procedure has been employed around each solution based on a concept of the \emph{extended} 1--perturbations. That is,   solutions which are in the vicinity of $s$ are generated w.r.t.\ the extended set of pre-defined per-node-changes, see again~\cite{kartelj2023rils}.  LS applies the best-improvement search strategy.  Thanks to the extended  per-node-changes, the LS procedure additionally performed an efficient and  important general coefficient tuning, unlike OLS that does the fitting only in linear combinations.  In the context of fitness function \emph{fit}, an effective way of calculation is performed utilizing the \emph{expression caching}. 
 
 \emph{Rils-Rols classifier}.  Now, it is necessary to turn the \textsc{Rils}-\textsc{Rols} algorithm, a state-of-the-art tool for solving the Symbolic regression problem, into a classifier. We do it for the problem of (symbolic) binary classification w.r.t.\ the value of a treshold parameter. More precisely, for a model $s$ obtained by executing the \textsc{Rils}-\textsc{Rols} algorithm evaluated on the historical data $D$, the concrete $\textbf{x}_i \in D$ is mapped to the corresponding target value of 1 if $s(\textbf{x}_i) \geq treshold$ or 0 otherwise. Interestingly, according to our experimental evaluation, the best \textit{accuracy} is  obtained by setting $treshold=0.5$. 
  
 \subsection{The \textsc{Hroch}  classifier}
 \fxnote{Marko: not all parameters will be mentioned -- will be adapted later on...}
\textit{ The HROCH parameters}  %https://github.com/janoPig/HROCH/blob/main/docs/HROCH.md
 \begin{itemize}
 	\item $pop_{size}$: Number of individuals in the population (default value is equal to 64).
 	\item $pop_{sel}$: The size of a tournament selection in each iteration (default value is equal to 4).
 	\item $code_{min size}, code_{max size}$: Minimum/maximum allowed size for a individual.
 	\item $const_{size}$: Maximum alloved constants in symbolic model.
 	\item $predefined_{const prob}$: Probability of selecting one of the predefined constants during equations mutation.
 	\item $predefined_{const set}$: Predefined constants used during equations mutation.
 	\item $problem$: Set of mathematical functions used in searched equations. Each mathematical function can have a defined weight with which it will be selected in the mutation process. By default, common mathematical operations such as multiplication and addition have a higher weight than goniometric functions or $pow$ and $\exp$. This is a natural way to eliminate $\sin(\sin(\exp(x)))$-type equations, which may have high precision and low complexity, but are usually inappropriate and difficult to interpret.
 	\item $feature_{probs}$: The probability that a mutation process will select a feature. This parameter allows using feature importances provided by a black-box regressor as an input parameter for symbolic regression to speed up the search by selecting mainly the important features.
 	\item $metric$: Metric used to verify goodness of solutions in the search process. Choose from MSE, MAE, MSLE, LogLoss.
 	\item $transformation$: Final transformation for computed value. Choose from logistic function for a classification tasks, no transformation for a regression problems, and ordinal(rounding) for a ordinal regression.
 	\item $sample_{weight}$: Array of weights that are assigned to individual samples.
 	\item $class_{weight}$: Weights associated with classes for a classification tasks with imbalanced classes distribution.
 \end{itemize}
 
\textit{ Stopping criteria}. The algorithm run until at least one of the two criteria is met:
 \begin{itemize}
 	\item  $time_{limit}$: Time limit is reached.
 	\item $iter_{limit}$: Number of iteration has exceeded.
 \end{itemize}

\textit{Fitness function}. To verify goodness of solutions for a classification task in the search process, we used $LogLoss$ function combined with a logistic transformation. For a regression task a $MSE$ function is utilized. That is:

\begin{equation}\label{eq:fitness-hroch-classification} 
	\texttt{LogLoss}(y, p) = -\frac{1}{N} \sum_{i=1}^{N} [ y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i) ] \cdot w_i \cdot c[y_i]
\end{equation}

\begin{equation}\label{eq:fitness-hroch-regression} 
\texttt{MSE}(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 \cdot w_i
\end{equation}

where $N$ is the number of elements (rows) of the dataset, $y_i$ is the ground truth (exact) value nad $\hat{y}_i$ is the predicted value for the $i$-th input vector, $p_i$ predicted probability of the positive class of the $i$-th input vector, $w_i$ sample weight, $c[y_i]$ class weight for a given class $y_i$. \fxnote{Jano, these weights $w_i$ and $c[y_i]$ are associated with the input params values $sample_{weight}$ and $class_{weight}$, right? J:Right. }

\textit{Solution representation}. Individual equations (solutions) in the search space  are represented as a fixed-length computer program encoded in three-address instructions code~\cite{oltean2002multi}, in the shape of $tmp_{i} = op_i(src_{1,i}, src_{2,i})$. Note that if  operation ${op}$ is unary, ${src}_{2,i}$ will not used. %The result of the last operation is taken as the global result. 
Not all instructions in given program must be used. Both $src_{1i}$ and $src_{2i}$ may point to one of variables, another instruction with lower index, or to one from constants. Each individual have a fixed number of constants controlled by algorithm parameter $const_{size}$. \fxnote{To all: I wonder if these details are necessary for readers J:I think so too.}
%\fxnote{Jano: TODO -- maybe citation to some Mihai Olteans work, his Multi Expression Programming use similat 3ops code}
 \\
\textit{Tournament selection}. Among the current population of solutions (models), the $pop_{sel}$ of them are selected in a random fashion and one with the best fitness score has been returned afterwards. This is know strategy applied in many evolutionary algorithms~\cite{blickle1995mathematical}. 

\textit{Mutation}. Each generation of neighbors consists of single code mutation and single constant mutation performed randomly. More in details, the code mutation selects one random instruction from the pre-defined instructions and randomly changes their mathematical operation or operand sources, or both. If the $src_{1i}$ or $src_{2i}$ refers to the result of another instruction, then with a $50\%$ probability the mutation is performed on that instruction as well. To mutate $op_i$, a weighted random sampling algorithm which use weights from $problem$ parameter given as input to algorithm. When a result of mutation $src_{1i}$ or $src_{2i}$ is a variable (feature) then we also apply the weighted random sampling with weights given in $feature_{probs}$ parameter. Basic constant mutation consists of multiplication or division of constant by $\delta$ value chosen from the following distribution

\begin{equation}\label{eq:hroch-mutation}
	\delta = 1 + \xi^4 + \epsilon, \quad \xi \sim U(0, 1),
	c_{new} = c_{old} * \delta \lor c_{new} = c_{old} / \delta
\end{equation}
\fxnote{Jano, how to motivate the usage of $\xi^4$. Why exactly the power of 4? J:In real cases, the constants can be very large or small, so adding/subtracting a small random value to them may not be effective. Therefore multiplying/dividing by some number. Power of 4, because I want to prioritize smaller changes. If the equation is in the correct form, many small mutations of the constants are needed to finetunig. Large changes are also important, but intuitively small changes need more space. $0.5^4$ is 0.0625, so $50\%$ of the mutations are smaller than $6\%$ of the value of the constant. I don't think this is an ideal function, maybe in the future I'll try something similar to the fibonachi sequence.}
where $\xi$ is a random variable in interval $(0, 1)$ whereas $\epsilon>0$ is a very small constant (fixed to $10^{-6}$). If a set of pre-defined constants $predefined_{const set}$ is passed to the algorithm, the mutation will with choose one constant randomly and with the probability  of $predefined_{const prob}>0$  and replace it with the current. 

\textit{Basic HROCH scheme}. The algorithm is based on the concept of hill climbing that is suited also to run in a parallel mode. The basic hill climbing algorithm~\cite{ohashi2003hill} is  a simple heuristic which belongs to the class of local search--based algorithms. In the basic hill climbing, the algorithm starts with an initial solution and then iteratively makes small modifications seeking for its improvements. The algorithm usually terminates when none of all possible small changes of the current best solution yield an improvement (of the objective value). 

Note that the HROCH algorithm, unlike basic hill climbing, works with a population of independent solutions that compete for the time allotted for their evolution (tournament selection). Like in the Tabu search, it improves the performance of local search by relaxing its basic rule. At each step, the best candidate replaces the previous solution unless there is a significant worsening of the score. Implementation of Tabu list for the SR problem can be challenging as the search space of SR is dual in its nature -- the discrete search part (finding a suitable equation i.e. the essential equation form) and the continuous part (fine-tuning the constants in the form). Instead, two complementary ideas are used. Choosing the best solution from $n$ generated neighboring solutions tends to the solution with the better score. Relaxing the strict rule that only a necessarily better solution must be followed avoids the algorithm getting stuck in a local minimum. It is controlled by $worsening_{limit}>0$ parameter.  

The basic pseudocode of the algorithm is given in Algorithm~\ref{alg:hroch}.

\begin{algorithm}
	\begin{algorithmic}[1]
		\Statex \textbf{Input}: Input training dataset $D_{tr}$
		\Statex \textbf{Control parameters}: $sample_{size}, pop_{sel}, neighbours_{count}, worsening_{limit}$ 
		\Statex \textbf{Output}: Symbolic solution (model) $bs$
		\State $s_{pop} \gets RandomIndividuals()$
		\For{$s \in s_{pop}$}
			\State $s.D_{tr}^{'} \gets Sample(D_{tr}, sample_{size})$
			\State $s.current.score^{'}, s.D_{tr}^{''} \gets Evaluate(s.current, s.D_{tr}^{'})$
			\Comment{$s.D_{tr}^{''}$ is a small portion of the data (the batch with size 64 rows) from sample $D_{tr}^{'}$ with the worst score.}
		\EndFor
        \While{! some of the termination criteria is met }
			\State $s \gets TournamentSelection(s_{pop}, pop_{sel})$
			\State $bestNeighbour \gets \emptyset$
            \For{$i = 0$ to $neighbours_{count}$} \Comment{try $neighbours_{count}$ neighbors of $s$}
				\State $neighbour \gets s.current$
				\State $DoMutation(neighbour)$
				\State $neighbour.score^{''} \gets Evaluate(neighbour, s.D_{tr}^{''})$
				\If{$neighbour.score^{''} < s.current.score^{''}*(1 + worsening_{limit})$}
					\State $neighbour.score^{'}, s.D_{tr}^{''} \gets Evaluate(neighbour, s.D_{tr}^{'})$
					\If{$neighbour.score^{'} < bestScore$}
						\State $bestNeighbour \gets neighbour$
					\EndIf
				\EndIf
            \EndFor
			\If{$bestNeighbour \neq \emptyset$ and $bestNeighbour.score^{'} < s.best.score{'} * (1 + worsening_{limit})$}
				\State $s.currentSolution \gets bestNeighbour$
				\If{$bestNeighbour.score^{'} < s.bestSolution.score{'}$}
					\State $s.bestSolution \gets bestNeighbour$
				\EndIf
			\EndIf
        \EndWhile
		\For{$s \in s_{pop}$} \Comment{retrieve the best solution from  $s_{pop}$}
			\State $s.score \gets Evaluate(s, D_{tr})$
			\If{$s.score < bs.score$}
				\State $bs \gets s$
			\EndIf
		\EndFor
		\State
		\Return{$bs$}
	\end{algorithmic}
    \caption{The basic scheme of HROCH.}
	\label{alg:hroch} 
\end{algorithm}


Jan \fxnote{o, what is individual here, maybe $s$ instead?It appears also at Line 22. bestScore a Line 15 used but never declared before J:Right, i forgot change it to $s$  } %https://github.com/janoPig/sr_core/tree/main/Hroch
 
 
 

 
\section{Experimental evaluation}\label{sec:experiments}

Our  algorithm is implemented in XX. All experiments concerning our method are conducted in the single-core mode, on a PC with Intel i9-9900KF CPU @3.6GHz, 64GB RAM, under Windows 10 Pro OS. The RAM consumption was very small (up to a few hundred megabytes).   

The following algorithms  are compared: XX...


\subsection{Datasets}
 XX
 
\section{Conclusions and future work}\label{sec:conclusions}
XX
 \fxnote{TODO}
\begin{backmatter}

 \section*{Acknowledgments}%% if any
Not applicable.

 \section*{Funding}%% if any
Not applicable.

%\section*{Abbreviations}%% if any
%Text for this section\ldots

  \section*{Availability of data and materials}%% if any
 All accompanying resources regarding this paper can be found in the GitHub repository \url{https://github.com/XX/XX}. 

\section*{Ethics approval and consent to participate}%% if any
Not applicable. 

\section*{Competing interests}
The authors declare  no competing interests.

\section*{Consent for publication}%% if any
Not applicable. 

\section*{Authors' contributions}
Both A.K. and M.Dj. contributed to the conception of the work. A.K. worked on the implementation of method and the design of experiments. M.Dj. worked on analysis and discussion of the results, introduction and literature review. 

%\section*{Authors' information}%% if any
%Text for this section\ldots

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bib}      % Bibliography file (usually '*.bib' )
 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files
 

 
 

%\section*{Additional Files}
%  \subsection*{Additional file 1 --- Sample additional file title}
 %   Additional file descriptions text (including details of how to
 %   view the file, if it is in a non-standard format or the file extension).  This might
 %   refer to a multi-page table or a figure...
 

\end{backmatter}
\end{document}
